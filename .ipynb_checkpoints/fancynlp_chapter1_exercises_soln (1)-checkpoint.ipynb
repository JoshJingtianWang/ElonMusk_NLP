{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h6OAMbQ1tRz"
   },
   "source": [
    "# Chapter 1 - spaCy Basics\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Run the cells with \"assert\" statements to see if your answer's output matches what the output should be. If it runs without error, your answer matches! If your output is different, you'll get a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vDSJZZE91kLy"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IORHUW7510XZ"
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "When working with spaCy, the first step is to choose and load a language model into your workspace.  Let's use spaCy's small English language model (\"en_core_web_sm\").  Use spaCy to load this model and save it as `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ISmghork1zJ1",
    "outputId": "75256568-8ce9-41f2-af84-ed75692e62b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you're gettting an error about being unable to find `en_core_web_sm`, uncomment and run this line first:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB9upond3oXQ"
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "Now let's use the language model `nlp` to parse a sentence.  We've included an exmaple sentence for you called `sent`.  \n",
    "1. Pass `sent` through the language model and save the output as `doc`.  \n",
    "2. Now use the token properties to create a list of all the adjective tokens of `doc` and call this list `adjectives`.  You may find a list comprehension to be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "URo79Fgl3fKY",
    "outputId": "b4565b3f-6a71-43bd-f1b8-40e407c44f3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[adorable, large, blue]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"The adorable kittens played with a large ball of blue yarn.\"\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "doc = nlp(sent)\n",
    "adjectives = [token for token in doc if token.pos_ == 'ADJ']\n",
    "### END SOLUTION\n",
    "\n",
    "adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OyhldetI4m7W"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "assert type(doc) == spacy.tokens.doc.Doc, \"Be sure that doc is a spacy document.  You should use the language model to parse the sentence provided.\"\n",
    "assert doc.text == sent, \"Be sure to use the language model to parse the sentence provided as sent.\"\n",
    "assert type(adjectives) == list, \"adjectives should be a Python list.\"\n",
    "assert type(adjectives[0]) == spacy.tokens.token.Token, \"Be sure that your adjectives lists contains spacy tokens as its elements.  The elements should not be strings.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wZIOXLzo5NwX"
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "test_doc = nlp(sent)\n",
    "test_adjs = [token for token in doc if token.pos_ == 'ADJ']\n",
    "assert len(doc) == len(test_doc)\n",
    "for token, test_token in zip(doc, test_doc):\n",
    "    assert token.pos_ == test_token.pos_\n",
    "for token, test_token in zip(adjectives, test_adjs):\n",
    "    assert token.pos_ == 'ADJ'\n",
    "    assert token.text == test_token.text\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnW0Z5vQ72OW"
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "Now we will use spaCy to do a bit of pre-processing.  Use `doc`, which is the parsed spaCy document for `sent`, to create a Python string that you will save as `sent_cleaned`.  `sent_cleaned` should not have any stop words or punctuation; furthermore, `sent_cleaned` should contain the remaining lemmatized, lowercase text from `sent`.  \n",
    "\n",
    "You will likely want to do the following to create `sent_cleaned`:\n",
    "1. Filter the stop words out of `doc`\n",
    "2. Filter the punctuation out of `doc` (remember that the punctuation part of speech is \"PUNCT\")\n",
    "3. Extract the lemmas from each of the remaining words (these will be lowercase)\n",
    "4. Save these lemmas together in a Python string (don't forget to join these with a space!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8rQDxWtd5Nyr",
    "outputId": "16fc4a8f-1efa-4b96-b0f0-3d9c0aeae9a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adorable kitten play large ball blue yarn'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "sent_cleaned = ''\n",
    "for token in doc[:-1]:\n",
    "    if not token.is_stop:\n",
    "        if not token.pos_ == 'PUNCT':\n",
    "            sent_cleaned += token.lemma_\n",
    "            sent_cleaned += ' '\n",
    "sent_cleaned = sent_cleaned.strip()\n",
    "### END SOLUTION\n",
    "\n",
    "sent_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yPHfliiC97E-"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "assert type(sent_cleaned) == str, \"Be sure that sent_cleaned is a Python string.\"\n",
    "assert len(sent_cleaned.split(' ')) == 7, \"You should find seven words that are not stop words or punctuation.  Be sure to separate these with spaces and that you do not have any trailing spaces.\"\n",
    "assert 'kittens' not in sent_cleaned, \"Be sure that your remaining text has been lemmatized.  The lemma for kittens should be kitten.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JNaqP6E6-yc3"
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TEST\n",
    "\n",
    "## allows them to forget to remove final space\n",
    "assert sent_cleaned.strip() == 'adorable kitten play large ball blue yarn'\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwFET74F_WsZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "fancynlp-chapter1-exercises-part2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
