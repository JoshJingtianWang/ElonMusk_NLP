{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLgXG6MUENUJ"
   },
   "source": [
    "# Chapter 3: Tokenization and the Document-Term Matrix\n",
    "\n",
    "## Instructions\n",
    "- Run the cells with \"assert\" statements to see if your answer's output matches what the output should be. If it runs without error, your answer matches! If your output is different, you'll get a hint.\n",
    "\n",
    "In this notebook, you'll practice turning raw text into a standard format using both tokenization and CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia3hGIa9cUSY"
   },
   "source": [
    "## 1. Tokenization\n",
    "\n",
    "In this section, you'll be testing out the `sent_tokenize`, `word_tokenize` and `RegexpTokenizer` functions in `nltk`. Make sure to note the differences in the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "foM40-kHJBHN",
    "outputId": "faadd583-4231-441b-a6c9-8cc93fd5490d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rita/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNDs-A3rO5Vr",
    "outputId": "207fa33f-9566-4f84-a2f3-c2c225a8860e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We strive, we sweat, we swear. We go the extra mile.         We stage, we fail. We try again. Get it right. We learn.         Connect. Come together. This is Metis. -12/9/2013\n"
     ]
    }
   ],
   "source": [
    "# Let's get some text to work with\n",
    "metis = 'We strive, we sweat, we swear. We go the extra mile.\\\n",
    "         We stage, we fail. We try again. Get it right. We learn.\\\n",
    "         Connect. Come together. This is Metis. -12/9/2013'\n",
    "print(metis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVQr2ekCNXQw"
   },
   "source": [
    "### Sentence Tokenization\n",
    "\n",
    "Tokenize the `metis` text by sentence. Save your results in a variable called `sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rZMdVAReNUJ4",
    "outputId": "f47ccc85-268b-401d-e712-2f44f60c5df0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We strive, we sweat, we swear.',\n",
       " 'We go the extra mile.',\n",
       " 'We stage, we fail.',\n",
       " 'We try again.',\n",
       " 'Get it right.',\n",
       " 'We learn.',\n",
       " 'Connect.',\n",
       " 'Come together.',\n",
       " 'This is Metis.',\n",
       " '-12/9/2013']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "sentences = sent_tokenize(metis)\n",
    "### END SOLUTION\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Blji-27LNUUx"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert type(sentences) == list, \"The output of sent_tokenize() should be a list.\"\n",
    "assert len(sentences) == 10, \"There should be ten items in the list. Hint: use sent_tokenize().\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmFVAIprPcY3"
   },
   "source": [
    "The final item in the list is not a sentence. Remove the item from the `sentences` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_pHrY6qPbMg",
    "outputId": "9f5cdea9-707a-4920-9520-2e503b45fd1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We strive, we sweat, we swear.',\n",
       " 'We go the extra mile.',\n",
       " 'We stage, we fail.',\n",
       " 'We try again.',\n",
       " 'Get it right.',\n",
       " 'We learn.',\n",
       " 'Connect.',\n",
       " 'Come together.',\n",
       " 'This is Metis.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "sentences.pop()\n",
    "### END SOLUTION\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5kYHbagMPbRV"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert type(sentences) == list, \"The output of sent_tokenize() should be a list.\"\n",
    "assert len(sentences) == 9, \"There should be nine sentences in the string. Hint: use sent_tokenize().\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3qo8rFvP8lE"
   },
   "source": [
    "### Word Tokenization\n",
    "\n",
    "Tokenize the `metis` text by word. First use `word_tokenize` and save your results in a variable called `words_wt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X02GEshsP8AN",
    "outputId": "53b5409e-92e0-4b0a-ed16-60684f9f5806"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'strive',\n",
       " ',',\n",
       " 'we',\n",
       " 'sweat',\n",
       " ',',\n",
       " 'we',\n",
       " 'swear',\n",
       " '.',\n",
       " 'We',\n",
       " 'go',\n",
       " 'the',\n",
       " 'extra',\n",
       " 'mile',\n",
       " '.',\n",
       " 'We',\n",
       " 'stage',\n",
       " ',',\n",
       " 'we',\n",
       " 'fail',\n",
       " '.',\n",
       " 'We',\n",
       " 'try',\n",
       " 'again',\n",
       " '.',\n",
       " 'Get',\n",
       " 'it',\n",
       " 'right',\n",
       " '.',\n",
       " 'We',\n",
       " 'learn',\n",
       " '.',\n",
       " 'Connect',\n",
       " '.',\n",
       " 'Come',\n",
       " 'together',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'Metis',\n",
       " '.',\n",
       " '-12/9/2013']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "words_wt = word_tokenize(metis)\n",
    "### END SOLUTION\n",
    "words_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HCt-Xm6EIOjL"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert type(words_wt) == list, \"The output of word_tokenize() should be a list.\"\n",
    "assert len(words_wt) == 42, \"There should be 42 items in the list.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-jGx2u0RKh3"
   },
   "source": [
    "Next use `RegexpTokenizer` to split on spaces (which is another way of tokenizing by word) and save your results in a variable called `words_re`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQkRKPQEIOoc",
    "outputId": "2f0a833d-a30d-44ea-cc37-f239e3524b20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'strive,',\n",
       " 'we',\n",
       " 'sweat,',\n",
       " 'we',\n",
       " 'swear.',\n",
       " 'We',\n",
       " 'go',\n",
       " 'the',\n",
       " 'extra',\n",
       " 'mile.',\n",
       " 'We',\n",
       " 'stage,',\n",
       " 'we',\n",
       " 'fail.',\n",
       " 'We',\n",
       " 'try',\n",
       " 'again.',\n",
       " 'Get',\n",
       " 'it',\n",
       " 'right.',\n",
       " 'We',\n",
       " 'learn.',\n",
       " 'Connect.',\n",
       " 'Come',\n",
       " 'together.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'Metis.',\n",
       " '-12/9/2013']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "words_re = RegexpTokenizer(\"\\s+\", gaps=True).tokenize(metis)\n",
    "### END SOLUTION\n",
    "words_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7RbPIqfyRbP-"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert type(words_re) == list, \"The output of RegexpTokenizer().tokenize() should be a list.\"\n",
    "assert len(words_re) == 30, \"There should be 30 items in the list.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtiUJL71RvJZ"
   },
   "source": [
    "Note the differences between `words_wt` and `words_re`, specifically how punctuation is treated. Let's try removing punctuation all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "5aNnsPzOR-dD",
    "outputId": "24049f38-ed56-4bbf-aa72-a24327bdaecc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We strive  we sweat  we swear  We go the extra mile          We stage  we fail  We try again  Get it right  We learn          Connect  Come together  This is Metis   12 9 2013'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "metis_no_punc = re.sub('[%s]' % re.escape(string.punctuation), ' ', metis)\n",
    "metis_no_punc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAt-OwYPTgWa"
   },
   "source": [
    "Tokenize `metis_no_punc` using `word_tokenize`. Save the variable as `words_wt_no_punc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tqMfg-BxIOtW",
    "outputId": "db9c0016-974e-478f-a5d9-8230a37aaeef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'strive',\n",
       " 'we',\n",
       " 'sweat',\n",
       " 'we',\n",
       " 'swear',\n",
       " 'We',\n",
       " 'go',\n",
       " 'the',\n",
       " 'extra',\n",
       " 'mile',\n",
       " 'We',\n",
       " 'stage',\n",
       " 'we',\n",
       " 'fail',\n",
       " 'We',\n",
       " 'try',\n",
       " 'again',\n",
       " 'Get',\n",
       " 'it',\n",
       " 'right',\n",
       " 'We',\n",
       " 'learn',\n",
       " 'Connect',\n",
       " 'Come',\n",
       " 'together',\n",
       " 'This',\n",
       " 'is',\n",
       " 'Metis',\n",
       " '12',\n",
       " '9',\n",
       " '2013']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "words_wt_no_punc = word_tokenize(metis_no_punc)\n",
    "### END SOLUTION\n",
    "words_wt_no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZdmmQadoUBAp"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert type(words_wt_no_punc) == list, \"The output of word_tokenize() should be a list.\"\n",
    "assert len(words_wt_no_punc) == 32, \"There should be 32 items in the list.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FevntlboUMq-"
   },
   "source": [
    "Tokenize `metis_no_punc` using `RegexpTokenizer`. Save the variable as `words_re_no_punc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-FnuW1flSa0V",
    "outputId": "af327813-2abd-4169-875c-e6a8632cc091"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'strive',\n",
       " 'we',\n",
       " 'sweat',\n",
       " 'we',\n",
       " 'swear',\n",
       " 'We',\n",
       " 'go',\n",
       " 'the',\n",
       " 'extra',\n",
       " 'mile',\n",
       " 'We',\n",
       " 'stage',\n",
       " 'we',\n",
       " 'fail',\n",
       " 'We',\n",
       " 'try',\n",
       " 'again',\n",
       " 'Get',\n",
       " 'it',\n",
       " 'right',\n",
       " 'We',\n",
       " 'learn',\n",
       " 'Connect',\n",
       " 'Come',\n",
       " 'together',\n",
       " 'This',\n",
       " 'is',\n",
       " 'Metis',\n",
       " '12',\n",
       " '9',\n",
       " '2013']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "words_re_no_punc = RegexpTokenizer(\"\\s+\", gaps=True).tokenize(metis_no_punc)\n",
    "### END SOLUTION\n",
    "words_re_no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MKmZI0lLT_rw"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert type(words_re_no_punc) == list, \"The output of RegexpTokenizer().tokenize() should be a list.\"\n",
    "assert len(words_re_no_punc) == 32, \"There should be 32 items in the list.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KWSPOL3Utp7"
   },
   "source": [
    "Note the differences between `words_wt_no_punc` and `words_re_no_punc`. Without the punctuation, the two functions resulted in the same list of words. While this won't always case, it is an example of how two tokenizers can result in the same word lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83ZY6hc5VE8d"
   },
   "source": [
    "### N-grams\n",
    "\n",
    "How many bi-grams are in the `metis_no_punc` string? Save the output as `num_bigrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuQIwIgJIOyQ",
    "outputId": "a84c8668-9fef-4bba-bdce-983eae2db159"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "num_bigrams = len(list(ngrams(word_tokenize(metis_no_punc),2)))\n",
    "### END SOLUTION\n",
    "num_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EaWxVG9sWBfl"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert num_bigrams == 31, \"That is not the correct number of bi-grams. Hint: ngrams(word_tokenize(text),2)\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdwshHDNVqpq"
   },
   "source": [
    "How many tri-grams are in the `metis_no_punc` string? Save the output as `num_trigrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRE7lNkrVp2i",
    "outputId": "31f217b6-43d4-4848-84a9-72faa2aba8f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "num_trigrams = len(list(ngrams(word_tokenize(metis_no_punc),3)))\n",
    "### END SOLUTION\n",
    "num_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iimyPDbKWcBH"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert num_trigrams == 30, \"That is not the correct number of tri-grams. Hint: ngrams(word_tokenize(text),3)\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I54BgZXegb7F"
   },
   "source": [
    "## 2. Document-Term Matrix\n",
    "\n",
    "In this section, you'll be playing around with `sklearn`'s `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "509QfROfW3Tt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "dHQR4DpbZiNj",
    "outputId": "e4ae90a2-d48f-4610-c6a2-fc6fcaf77c81"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>stars</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>5</td>\n",
       "      <td>Grove Square Cappuccino Cups were excellent. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>I love my Keurig, and I love most of the Keuri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>It's a powdered drink. No filter in k-cup.&lt;br ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>don't bother! bet you couldn't tell the differ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>1</td>\n",
       "      <td>Never tasted this coffee before, I felt much t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f</td>\n",
       "      <td>5</td>\n",
       "      <td>My husband and I LOVE this French Vanilla Capp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  users  stars                                            reviews\n",
       "0     a      5  Grove Square Cappuccino Cups were excellent. T...\n",
       "1     b      1  I love my Keurig, and I love most of the Keuri...\n",
       "2     c      1  It's a powdered drink. No filter in k-cup.<br ...\n",
       "3     d      1  don't bother! bet you couldn't tell the differ...\n",
       "4     e      1  Never tasted this coffee before, I felt much t...\n",
       "5     f      5  My husband and I LOVE this French Vanilla Capp..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's set up a dataframe to work with\n",
    "df = pd.DataFrame([['a',5,\"Grove Square Cappuccino Cups were excellent. Tasted really good right from the Keurig brewer with nothing added. wWould highly recommend. RCCJR\"],\n",
    "                  ['b',1,\"I love my Keurig, and I love most of the Keurig coffees. This is instant coffee with instant milk and far too much sugar. I don't know anyone I dislike enough to dump the rest of the box on.\"],\n",
    "                  ['c',1,\"It's a powdered drink. No filter in k-cup.<br />Just buy it in bulk and mix it with hot water....<br /><br />Nothing else to say here. Wont be buying it again.\"],\n",
    "                  ['d',1,\"don't bother! bet you couldn't tell the difference between this and hot water if your eyes were closed. well, maybe the water would have a taste!\"],\n",
    "                  ['e',1,\"Never tasted this coffee before, I felt much too sweet even for dessert. I would not order again. But then that is only my opinion. My friend's husband loves it.<br />I gave them to him.\"],\n",
    "                  ['f',5,\"My husband and I LOVE this French Vanilla Cappuccino. Sooo glad I didn't listen to some of the reviews and took the plunge and bought it.\"]],\n",
    "                  columns=['users','stars','reviews'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sXCt4VoZjyU",
    "outputId": "cbedb358-02c8-498f-9b2d-6c935619d4d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Grove Square Cappuccino Cups were excellent. Tasted really good right from the Keurig brewer with nothing added. wWould highly recommend. RCCJR',\n",
       " \"I love my Keurig, and I love most of the Keurig coffees. This is instant coffee with instant milk and far too much sugar. I don't know anyone I dislike enough to dump the rest of the box on.\",\n",
       " \"It's a powdered drink. No filter in k-cup.<br />Just buy it in bulk and mix it with hot water....<br /><br />Nothing else to say here. Wont be buying it again.\",\n",
       " \"don't bother! bet you couldn't tell the difference between this and hot water if your eyes were closed. well, maybe the water would have a taste!\",\n",
       " \"Never tasted this coffee before, I felt much too sweet even for dessert. I would not order again. But then that is only my opinion. My friend's husband loves it.<br />I gave them to him.\",\n",
       " \"My husband and I LOVE this French Vanilla Cappuccino. Sooo glad I didn't listen to some of the reviews and took the plunge and bought it.\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = list(df.reviews)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYox2wW7byak"
   },
   "source": [
    "Create a document-term matrix from the `corpus` text using `CountVectorizer` and `pd.DataFrame`. Save the results in a variable called `dtm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "-eJ_rhTdW3et",
    "outputId": "ce022c69-95b3-48e1-9023-86b46e92f71f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>added</th>\n",
       "      <th>again</th>\n",
       "      <th>and</th>\n",
       "      <th>anyone</th>\n",
       "      <th>be</th>\n",
       "      <th>before</th>\n",
       "      <th>bet</th>\n",
       "      <th>between</th>\n",
       "      <th>bother</th>\n",
       "      <th>bought</th>\n",
       "      <th>...</th>\n",
       "      <th>vanilla</th>\n",
       "      <th>water</th>\n",
       "      <th>well</th>\n",
       "      <th>were</th>\n",
       "      <th>with</th>\n",
       "      <th>wont</th>\n",
       "      <th>would</th>\n",
       "      <th>wwould</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   added  again  and  anyone  be  before  bet  between  bother  bought  ...  \\\n",
       "0      1      0    0       0   0       0    0        0       0       0  ...   \n",
       "1      0      0    2       1   0       0    0        0       0       0  ...   \n",
       "2      0      1    1       0   1       0    0        0       0       0  ...   \n",
       "3      0      0    1       0   0       0    1        1       1       0  ...   \n",
       "4      0      1    0       0   0       1    0        0       0       0  ...   \n",
       "5      0      0    3       0   0       0    0        0       0       1  ...   \n",
       "\n",
       "   vanilla  water  well  were  with  wont  would  wwould  you  your  \n",
       "0        0      0     0     1     1     0      0       1    0     0  \n",
       "1        0      0     0     0     1     0      0       0    0     0  \n",
       "2        0      1     0     0     1     1      0       0    0     0  \n",
       "3        0      2     1     1     0     0      1       0    1     1  \n",
       "4        0      0     0     0     0     0      1       0    0     0  \n",
       "5        1      0     0     0     0     0      0       0    0     0  \n",
       "\n",
       "[6 rows x 114 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus)\n",
    "dtm = pd.DataFrame(X.toarray(), columns=cv.get_feature_names())\n",
    "### END SOLUTION\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "419qgYM3aR8f"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert type(dtm) == pd.DataFrame, \"The output should be a DataFrame.\"\n",
    "assert dtm.shape == (6,114), \"The shape of the document-term matrix should be 6 rows x 114 columns.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBKn0O1_cTzA"
   },
   "source": [
    "This document-term matrix has a lot of columns. Remove all of the stop words from the dataset by updating the stop words hyperparameter in Count Vectorizer with `CountVectorizer(stop_words = 'english')`. Save the updated dataframe in a variable called `dtm2`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "pGrks69zW3kP",
    "outputId": "beb767ab-b1d5-4944-e904-e779574d6b28"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>added</th>\n",
       "      <th>bet</th>\n",
       "      <th>bother</th>\n",
       "      <th>bought</th>\n",
       "      <th>box</th>\n",
       "      <th>br</th>\n",
       "      <th>brewer</th>\n",
       "      <th>bulk</th>\n",
       "      <th>buy</th>\n",
       "      <th>buying</th>\n",
       "      <th>...</th>\n",
       "      <th>sugar</th>\n",
       "      <th>sweet</th>\n",
       "      <th>taste</th>\n",
       "      <th>tasted</th>\n",
       "      <th>tell</th>\n",
       "      <th>took</th>\n",
       "      <th>vanilla</th>\n",
       "      <th>water</th>\n",
       "      <th>wont</th>\n",
       "      <th>wwould</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   added  bet  bother  bought  box  br  brewer  bulk  buy  buying  ...  sugar  \\\n",
       "0      1    0       0       0    0   0       1     0    0       0  ...      0   \n",
       "1      0    0       0       0    1   0       0     0    0       0  ...      1   \n",
       "2      0    0       0       0    0   3       0     1    1       1  ...      0   \n",
       "3      0    1       1       0    0   0       0     0    0       0  ...      0   \n",
       "4      0    0       0       0    0   1       0     0    0       0  ...      0   \n",
       "5      0    0       0       1    0   0       0     0    0       0  ...      0   \n",
       "\n",
       "   sweet  taste  tasted  tell  took  vanilla  water  wont  wwould  \n",
       "0      0      0       1     0     0        0      0     0       1  \n",
       "1      0      0       0     0     0        0      0     0       0  \n",
       "2      0      0       0     0     0        0      1     1       0  \n",
       "3      0      1       0     1     0        0      2     0       0  \n",
       "4      1      0       1     0     0        0      0     0       0  \n",
       "5      0      0       0     0     1        1      0     0       0  \n",
       "\n",
       "[6 rows x 71 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "cv2 = CountVectorizer(stop_words = 'english')\n",
    "X2 = cv2.fit_transform(corpus)\n",
    "dtm2 = pd.DataFrame(X2.toarray(), columns=cv2.get_feature_names())\n",
    "### END SOLUTION\n",
    "dtm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "RuOFKwjebDQE"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert type(dtm2) == pd.DataFrame, \"The output should be a DataFrame.\"\n",
    "assert dtm2.shape == (6,71), \"The shape of the document-term matrix should be 6 rows x 71 columns.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjjFjZlLdbFC"
   },
   "source": [
    "This document-term matrix still has a good number of columns. In addition to removing all of the stop words, let's also only keep words that occur in more than one document. We have six documents, so one document is 16.7% all of our documents. So to occur in more than one document, a word would need to occur in more than 16.7% of the documents we have.\n",
    "\n",
    "Update the minimum document frequency (`min_df`) hyperparameter to be .1667 in Count Vectorizer with `CountVectorizer(stop_words = 'english', min_df = .1667)`. Save the updated dataframe in a variable called `dtm3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "JpGxWBIEZCcY",
    "outputId": "5dcdb39e-5558-40b0-fac9-d74f57f05d1e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>br</th>\n",
       "      <th>cappuccino</th>\n",
       "      <th>coffee</th>\n",
       "      <th>don</th>\n",
       "      <th>hot</th>\n",
       "      <th>husband</th>\n",
       "      <th>keurig</th>\n",
       "      <th>love</th>\n",
       "      <th>tasted</th>\n",
       "      <th>water</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   br  cappuccino  coffee  don  hot  husband  keurig  love  tasted  water\n",
       "0   0           1       0    0    0        0       1     0       1      0\n",
       "1   0           0       1    1    0        0       2     2       0      0\n",
       "2   3           0       0    0    1        0       0     0       0      1\n",
       "3   0           0       0    1    1        0       0     0       0      2\n",
       "4   1           0       1    0    0        1       0     0       1      0\n",
       "5   0           1       0    0    0        1       0     1       0      0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "cv3 = CountVectorizer(stop_words = 'english', min_df = .1667)\n",
    "X3 = cv3.fit_transform(corpus)\n",
    "dtm3 = pd.DataFrame(X3.toarray(), columns=cv3.get_feature_names())\n",
    "### END SOLUTION\n",
    "\n",
    "dtm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Oc4SeAomZ8XC"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert type(dtm3) == pd.DataFrame, \"The output should be a DataFrame.\"\n",
    "assert dtm3.shape == (6,10), \"The shape of the document-term matrix should be 6 rows x 10 columns.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nY834Yxgq5O"
   },
   "source": [
    "The main takeaway here is that you can create a document-term matrix using Count Vectorizer, and you can also fine tune the terms in your matrix by adjusting the hyperparameters. Common hyperparameters to tune are:\n",
    "* `stop_words`: in addition to the standard English list, you can also add custom stop words to the list\n",
    "* `ngram_range`: (1,3) would mean include terms that are unigrams, bi-grams and tri-grams\n",
    "* `min_df`: minimum document frequency, with a range between 0 and 1, helps you filter out uncommon words\n",
    "* `max_df`: maximum document frequency, with a range between 0 and 1, helps you filter out common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUvJ2bmIbecj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nlp-unsupervised-nlp-exercises3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
