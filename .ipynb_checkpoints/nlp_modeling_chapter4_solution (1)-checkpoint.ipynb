{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLgXG6MUENUJ"
   },
   "source": [
    "# Chapter 4: Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Run the cells with \"assert\" statements to see if your answer's output matches what the output should be. If it runs without error, your answer matches! If your output is different, you'll get a hint.\n",
    "\n",
    "In this notebook, you'll get to practice topic modeling with LDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NHOyO9BeSXEx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim import corpora, models, matutils\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_A few notes:_\n",
    "\n",
    "_You can run first the line of code below if you'd like to suppress a warning that appears upon importing gensim._\n",
    "\n",
    "_Additionally, if you get an error about numpy or cannot run the code above, it is related to importing gensim and is because you have an older version of numpy on your computer. Run the code in the second cell, restart your kernel, and that should remove the older version of numpy and install the latest one. All of this code is currently commented out._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall numpy -y\n",
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3ChnL9ri0j2"
   },
   "source": [
    "We are going to be using data from the famous 20 news groups dataset, specifically the motorcycles data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "tzmcy-XEXiDM",
    "outputId": "46ae85c2-6651-4963-a182-f8e49bd95d0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Now, I am jumping into the middle of this thread so I may not know\\nwhat y'all been talking about, but I have a few comments:\\n\\n\\nThere are a number of other factors that are very important, the three\\nbiggest being air velocity, air momentum and shock waves.\\nVelocity stacks have been used for years and are now being used inside\\nof stock airboxes on a number of bikes.  At a tuned engine rpm, the\\nstacks can greatly increase the speed, and thus momentum of the air\\nrushing in.\\nAir momentum is critical in getting good air intake: the momentum of\\nthe air stack outside the combustion chamber will force its way inside\\nlong after the piston has begun its compressive up-stroke.\\nShock waves are used to induce air intake and to prevent fresh air from\\nescaping out the exzhaust ports.  Shock waves are the product of expansion\\nchambers or any other means of presenting a 'wall' (opening or closing)\\nto the air in motion.  Beyond this I am lost in the mystery of how they\\ndesign for shock waves.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = datasets.fetch_20newsgroups(subset='train', categories=['rec.motorcycles'], remove=('headers', 'footers', 'quotes'))\n",
    "news.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihqRPrUdjBWZ"
   },
   "source": [
    "Before topic modeling, the first step is to get your text data in a format that is ready for modeling. Use `CountVectorizer` with English stop words and both unigrams and bigrams to turn the corpus into a document-term matrix. Save the matrix as `doc_term`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xh5PpRzQXiHJ",
    "outputId": "ba3f499d-5752-457d-f517-7617b75ef89c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598, 34997)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "doc_term = vectorizer.fit_transform(news.data)\n",
    "### END SOLUTION\n",
    "doc_term.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BS99pyGbjxQy"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert doc_term.shape == (598, 34997), \"The doc_term matrix should have 598 documents and 34997 terms.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_gaHrf4jwxv"
   },
   "source": [
    "Turn the `doc_term` matrix into a dataframe. Modify the code below to do so and save the output as `doc_term_df`.\n",
    "\n",
    "```\n",
    "pd.DataFrame(INSERT_VALUE_HERE.toarray(), index=ex_label, columns=vectorizer.get_feature_names())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rita/opt/anaconda3/envs/metis/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 23</th>\n",
       "      <th>00 42</th>\n",
       "      <th>00 battery</th>\n",
       "      <th>00 best</th>\n",
       "      <th>00 clothing</th>\n",
       "      <th>00 evening</th>\n",
       "      <th>00 pm</th>\n",
       "      <th>00 wasn</th>\n",
       "      <th>000</th>\n",
       "      <th>...</th>\n",
       "      <th>zx cbr</th>\n",
       "      <th>zx engine</th>\n",
       "      <th>zx spend</th>\n",
       "      <th>zx tried</th>\n",
       "      <th>zx900</th>\n",
       "      <th>zx900 payload</th>\n",
       "      <th>zx900a</th>\n",
       "      <th>zx900a supertrapp</th>\n",
       "      <th>zygot</th>\n",
       "      <th>zygot ati</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>598 rows Ã— 34997 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     00  00 23  00 42  00 battery  00 best  00 clothing  00 evening  00 pm  \\\n",
       "0     0      0      0           0        0            0           0      0   \n",
       "1     0      0      0           0        0            0           0      0   \n",
       "2     0      0      0           0        0            0           0      0   \n",
       "3     0      0      0           0        0            0           0      0   \n",
       "4     0      0      0           0        0            0           0      0   \n",
       "..   ..    ...    ...         ...      ...          ...         ...    ...   \n",
       "593   0      0      0           0        0            0           0      0   \n",
       "594   0      0      0           0        0            0           0      0   \n",
       "595   0      0      0           0        0            0           0      0   \n",
       "596   0      0      0           0        0            0           0      0   \n",
       "597   0      0      0           0        0            0           0      0   \n",
       "\n",
       "     00 wasn  000  ...  zx cbr  zx engine  zx spend  zx tried  zx900  \\\n",
       "0          0    0  ...       0          0         0         0      0   \n",
       "1          0    0  ...       0          0         0         0      0   \n",
       "2          0    0  ...       0          0         0         0      0   \n",
       "3          0    0  ...       0          0         0         0      0   \n",
       "4          0    1  ...       0          0         0         0      0   \n",
       "..       ...  ...  ...     ...        ...       ...       ...    ...   \n",
       "593        0    0  ...       0          0         0         0      0   \n",
       "594        0    0  ...       0          0         0         0      0   \n",
       "595        0    0  ...       0          0         0         0      0   \n",
       "596        0    0  ...       0          0         0         0      0   \n",
       "597        0    0  ...       0          0         0         0      0   \n",
       "\n",
       "     zx900 payload  zx900a  zx900a supertrapp  zygot  zygot ati  \n",
       "0                0       0                  0      0          0  \n",
       "1                0       0                  0      0          0  \n",
       "2                0       0                  0      0          0  \n",
       "3                0       0                  0      0          0  \n",
       "4                0       0                  0      0          0  \n",
       "..             ...     ...                ...    ...        ...  \n",
       "593              0       0                  0      0          0  \n",
       "594              0       0                  0      0          0  \n",
       "595              0       0                  0      0          0  \n",
       "596              0       0                  0      0          0  \n",
       "597              0       0                  0      0          0  \n",
       "\n",
       "[598 rows x 34997 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "doc_term_df = pd.DataFrame(doc_term.toarray(), columns=vectorizer.get_feature_names())\n",
    "# ### END SOLUTION\n",
    "doc_term_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jG0Niy1hYhHK"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert type(doc_term_df) == pd.DataFrame, \"The output should be a dataframe.\"\n",
    "assert doc_term_df.shape == (598, 34997), \"The output should have 598 documents and 34997 terms.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xX3KcvDrlhhO"
   },
   "source": [
    "Fit an LDA model using `LdaModel` with three topics. Set the `passes` hyperparameter to 10 so that the corpus will be scanned 10 times. Save the fitted model as `lda`.\n",
    "\n",
    "NOTE: This may take a few minutes to run. Take a look at the log while you're waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bV5L6gXX4Ey",
    "outputId": "3268cf17-afb3-4299-cf87-803072348125"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 17:14:53,006 : INFO : using symmetric alpha at 0.3333333333333333\n",
      "2021-11-11 17:14:53,007 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2021-11-11 17:14:53,013 : INFO : using serial LDA version on this node\n",
      "2021-11-11 17:14:53,027 : INFO : running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 598 documents, updating model once every 598 documents, evaluating perplexity every 598 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2021-11-11 17:14:53,769 : INFO : -11.654 per-word bound, 3222.1 perplexity estimate based on a held-out corpus of 598 documents with 61523 words\n",
      "2021-11-11 17:14:53,770 : INFO : PROGRESS: pass 0, at document #598/598\n",
      "2021-11-11 17:14:54,151 : INFO : topic #0 (0.333): 0.004*\"bike\" + 0.002*\"just\" + 0.002*\"like\" + 0.002*\"dod\" + 0.001*\"ride\" + 0.001*\"don\" + 0.001*\"time\" + 0.001*\"know\" + 0.001*\"riding\" + 0.001*\"good\"\n",
      "2021-11-11 17:14:54,153 : INFO : topic #1 (0.333): 0.003*\"bike\" + 0.002*\"dod\" + 0.002*\"like\" + 0.002*\"good\" + 0.002*\"motorcycle\" + 0.002*\"think\" + 0.002*\"right\" + 0.001*\"know\" + 0.001*\"just\" + 0.001*\"new\"\n",
      "2021-11-11 17:14:54,154 : INFO : topic #2 (0.333): 0.004*\"bike\" + 0.003*\"just\" + 0.002*\"like\" + 0.002*\"dod\" + 0.002*\"don\" + 0.002*\"know\" + 0.002*\"ride\" + 0.001*\"helmet\" + 0.001*\"bikes\" + 0.001*\"ve\"\n",
      "2021-11-11 17:14:54,156 : INFO : topic diff=0.916317, rho=1.000000\n",
      "2021-11-11 17:14:54,740 : INFO : -10.640 per-word bound, 1595.3 perplexity estimate based on a held-out corpus of 598 documents with 61523 words\n",
      "2021-11-11 17:14:54,741 : INFO : PROGRESS: pass 1, at document #598/598\n",
      "2021-11-11 17:14:54,946 : INFO : topic #0 (0.333): 0.004*\"bike\" + 0.002*\"just\" + 0.002*\"like\" + 0.001*\"dod\" + 0.001*\"don\" + 0.001*\"ride\" + 0.001*\"good\" + 0.001*\"know\" + 0.001*\"riding\" + 0.001*\"way\"\n",
      "2021-11-11 17:14:54,948 : INFO : topic #1 (0.333): 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"motorcycle\" + 0.002*\"dod\" + 0.002*\"know\" + 0.001*\"good\" + 0.001*\"right\" + 0.001*\"think\" + 0.001*\"just\" + 0.001*\"bmw\"\n",
      "2021-11-11 17:14:54,950 : INFO : topic #2 (0.333): 0.004*\"bike\" + 0.003*\"dod\" + 0.002*\"just\" + 0.002*\"like\" + 0.002*\"don\" + 0.002*\"ride\" + 0.002*\"know\" + 0.001*\"helmet\" + 0.001*\"ve\" + 0.001*\"good\"\n",
      "2021-11-11 17:14:54,951 : INFO : topic diff=0.479908, rho=0.577350\n",
      "2021-11-11 17:14:55,478 : INFO : -10.399 per-word bound, 1350.6 perplexity estimate based on a held-out corpus of 598 documents with 61523 words\n",
      "2021-11-11 17:14:55,479 : INFO : PROGRESS: pass 2, at document #598/598\n",
      "2021-11-11 17:14:55,628 : INFO : topic #0 (0.333): 0.004*\"bike\" + 0.002*\"just\" + 0.002*\"like\" + 0.001*\"good\" + 0.001*\"don\" + 0.001*\"know\" + 0.001*\"got\" + 0.001*\"dod\" + 0.001*\"ride\" + 0.001*\"way\"\n",
      "2021-11-11 17:14:55,630 : INFO : topic #1 (0.333): 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"motorcycle\" + 0.002*\"just\" + 0.002*\"know\" + 0.001*\"right\" + 0.001*\"think\" + 0.001*\"good\" + 0.001*\"bmw\" + 0.001*\"don\"\n",
      "2021-11-11 17:14:55,632 : INFO : topic #2 (0.333): 0.004*\"bike\" + 0.003*\"dod\" + 0.002*\"just\" + 0.002*\"like\" + 0.002*\"don\" + 0.002*\"ride\" + 0.002*\"know\" + 0.001*\"helmet\" + 0.001*\"ve\" + 0.001*\"dog\"\n",
      "2021-11-11 17:14:55,633 : INFO : topic diff=0.290842, rho=0.500000\n",
      "2021-11-11 17:14:56,135 : INFO : -10.309 per-word bound, 1268.9 perplexity estimate based on a held-out corpus of 598 documents with 61523 words\n",
      "2021-11-11 17:14:56,136 : INFO : PROGRESS: pass 3, at document #598/598\n",
      "2021-11-11 17:14:56,265 : INFO : topic #0 (0.333): 0.004*\"bike\" + 0.002*\"just\" + 0.002*\"like\" + 0.001*\"good\" + 0.001*\"don\" + 0.001*\"got\" + 0.001*\"know\" + 0.001*\"way\" + 0.001*\"helmet\" + 0.001*\"dod\"\n",
      "2021-11-11 17:14:56,266 : INFO : topic #1 (0.333): 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"motorcycle\" + 0.002*\"know\" + 0.001*\"right\" + 0.001*\"bmw\" + 0.001*\"think\" + 0.001*\"bikes\" + 0.001*\"good\"\n",
      "2021-11-11 17:14:56,267 : INFO : topic #2 (0.333): 0.004*\"dod\" + 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"don\" + 0.002*\"ride\" + 0.002*\"know\" + 0.001*\"dog\" + 0.001*\"helmet\" + 0.001*\"ve\"\n",
      "2021-11-11 17:14:56,269 : INFO : topic diff=0.161608, rho=0.447214\n",
      "2021-11-11 17:14:56,725 : INFO : -10.280 per-word bound, 1243.7 perplexity estimate based on a held-out corpus of 598 documents with 61523 words\n",
      "2021-11-11 17:14:56,726 : INFO : PROGRESS: pass 4, at document #598/598\n",
      "2021-11-11 17:14:56,853 : INFO : topic #0 (0.333): 0.004*\"bike\" + 0.002*\"just\" + 0.002*\"like\" + 0.001*\"good\" + 0.001*\"don\" + 0.001*\"got\" + 0.001*\"know\" + 0.001*\"helmet\" + 0.001*\"way\" + 0.001*\"dod\"\n",
      "2021-11-11 17:14:56,854 : INFO : topic #1 (0.333): 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"motorcycle\" + 0.002*\"know\" + 0.001*\"right\" + 0.001*\"bikes\" + 0.001*\"bmw\" + 0.001*\"think\" + 0.001*\"don\"\n",
      "2021-11-11 17:14:56,855 : INFO : topic #2 (0.333): 0.004*\"dod\" + 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"don\" + 0.002*\"ride\" + 0.002*\"know\" + 0.001*\"dog\" + 0.001*\"ve\" + 0.001*\"time\"\n",
      "2021-11-11 17:14:56,856 : INFO : topic diff=0.092717, rho=0.408248\n",
      "2021-11-11 17:14:57,291 : INFO : -10.270 per-word bound, 1234.9 perplexity estimate based on a held-out corpus of 598 documents with 61523 words\n",
      "2021-11-11 17:14:57,292 : INFO : PROGRESS: pass 5, at document #598/598\n",
      "2021-11-11 17:14:57,409 : INFO : topic #0 (0.333): 0.004*\"bike\" + 0.002*\"just\" + 0.002*\"like\" + 0.001*\"good\" + 0.001*\"don\" + 0.001*\"got\" + 0.001*\"know\" + 0.001*\"helmet\" + 0.001*\"way\" + 0.001*\"dod\"\n",
      "2021-11-11 17:14:57,410 : INFO : topic #1 (0.333): 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"motorcycle\" + 0.002*\"know\" + 0.001*\"right\" + 0.001*\"bikes\" + 0.001*\"bmw\" + 0.001*\"don\" + 0.001*\"think\"\n",
      "2021-11-11 17:14:57,412 : INFO : topic #2 (0.333): 0.004*\"dod\" + 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"ride\" + 0.002*\"don\" + 0.002*\"know\" + 0.001*\"dog\" + 0.001*\"time\" + 0.001*\"ve\"\n",
      "2021-11-11 17:14:57,414 : INFO : topic diff=0.055131, rho=0.377964\n",
      "2021-11-11 17:14:57,827 : INFO : -10.266 per-word bound, 1231.5 perplexity estimate based on a held-out corpus of 598 documents with 61523 words\n",
      "2021-11-11 17:14:57,828 : INFO : PROGRESS: pass 6, at document #598/598\n",
      "2021-11-11 17:14:57,942 : INFO : topic #0 (0.333): 0.004*\"bike\" + 0.002*\"just\" + 0.002*\"like\" + 0.001*\"good\" + 0.001*\"don\" + 0.001*\"got\" + 0.001*\"helmet\" + 0.001*\"know\" + 0.001*\"way\" + 0.001*\"dod\"\n",
      "2021-11-11 17:14:57,943 : INFO : topic #1 (0.333): 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"motorcycle\" + 0.002*\"know\" + 0.001*\"bikes\" + 0.001*\"right\" + 0.001*\"bmw\" + 0.001*\"don\" + 0.001*\"think\"\n",
      "2021-11-11 17:14:57,944 : INFO : topic #2 (0.333): 0.004*\"dod\" + 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"ride\" + 0.002*\"don\" + 0.002*\"know\" + 0.001*\"dog\" + 0.001*\"time\" + 0.001*\"ve\"\n",
      "2021-11-11 17:14:57,946 : INFO : topic diff=0.034133, rho=0.353553\n",
      "2021-11-11 17:14:58,386 : INFO : -10.264 per-word bound, 1229.8 perplexity estimate based on a held-out corpus of 598 documents with 61523 words\n",
      "2021-11-11 17:14:58,386 : INFO : PROGRESS: pass 7, at document #598/598\n",
      "2021-11-11 17:14:58,502 : INFO : topic #0 (0.333): 0.004*\"bike\" + 0.002*\"just\" + 0.002*\"like\" + 0.001*\"good\" + 0.001*\"don\" + 0.001*\"got\" + 0.001*\"helmet\" + 0.001*\"know\" + 0.001*\"way\" + 0.001*\"dod\"\n",
      "2021-11-11 17:14:58,504 : INFO : topic #1 (0.333): 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"know\" + 0.002*\"motorcycle\" + 0.001*\"bikes\" + 0.001*\"right\" + 0.001*\"bmw\" + 0.001*\"don\" + 0.001*\"think\"\n",
      "2021-11-11 17:14:58,505 : INFO : topic #2 (0.333): 0.004*\"dod\" + 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"ride\" + 0.002*\"don\" + 0.002*\"know\" + 0.001*\"dog\" + 0.001*\"time\" + 0.001*\"ve\"\n",
      "2021-11-11 17:14:58,506 : INFO : topic diff=0.021743, rho=0.333333\n",
      "2021-11-11 17:14:58,925 : INFO : -10.263 per-word bound, 1228.9 perplexity estimate based on a held-out corpus of 598 documents with 61523 words\n",
      "2021-11-11 17:14:58,925 : INFO : PROGRESS: pass 8, at document #598/598\n",
      "2021-11-11 17:14:59,040 : INFO : topic #0 (0.333): 0.004*\"bike\" + 0.002*\"just\" + 0.002*\"like\" + 0.001*\"good\" + 0.001*\"don\" + 0.001*\"got\" + 0.001*\"helmet\" + 0.001*\"know\" + 0.001*\"way\" + 0.001*\"dod\"\n",
      "2021-11-11 17:14:59,041 : INFO : topic #1 (0.333): 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"know\" + 0.002*\"motorcycle\" + 0.001*\"bikes\" + 0.001*\"right\" + 0.001*\"bmw\" + 0.001*\"don\" + 0.001*\"think\"\n",
      "2021-11-11 17:14:59,042 : INFO : topic #2 (0.333): 0.004*\"dod\" + 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"ride\" + 0.002*\"don\" + 0.002*\"know\" + 0.001*\"dog\" + 0.001*\"time\" + 0.001*\"ve\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 17:14:59,044 : INFO : topic diff=0.014165, rho=0.316228\n",
      "2021-11-11 17:14:59,466 : INFO : -10.263 per-word bound, 1228.4 perplexity estimate based on a held-out corpus of 598 documents with 61523 words\n",
      "2021-11-11 17:14:59,467 : INFO : PROGRESS: pass 9, at document #598/598\n",
      "2021-11-11 17:14:59,587 : INFO : topic #0 (0.333): 0.004*\"bike\" + 0.002*\"just\" + 0.002*\"like\" + 0.001*\"good\" + 0.001*\"don\" + 0.001*\"got\" + 0.001*\"helmet\" + 0.001*\"know\" + 0.001*\"way\" + 0.001*\"dod\"\n",
      "2021-11-11 17:14:59,588 : INFO : topic #1 (0.333): 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"know\" + 0.002*\"motorcycle\" + 0.001*\"bikes\" + 0.001*\"right\" + 0.001*\"bmw\" + 0.001*\"don\" + 0.001*\"think\"\n",
      "2021-11-11 17:14:59,589 : INFO : topic #2 (0.333): 0.004*\"dod\" + 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"ride\" + 0.002*\"don\" + 0.002*\"know\" + 0.001*\"dog\" + 0.001*\"time\" + 0.001*\"ve\"\n",
      "2021-11-11 17:14:59,590 : INFO : topic diff=0.009487, rho=0.301511\n",
      "2021-11-11 17:14:59,594 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel(num_terms=34997, num_topics=3, decay=0.5, chunksize=2000) in 6.57s', 'datetime': '2021-11-11T17:14:59.594581', 'gensim': '4.0.1', 'python': '3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 20:33:18) \\n[Clang 11.1.0 ]', 'platform': 'macOS-12.0.1-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x11b2bd580>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "term_doc = doc_term.transpose()\n",
    "corpus = matutils.Sparse2Corpus(term_doc)\n",
    "id2word = dict((v, k) for k, v in vectorizer.vocabulary_.items())\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=3, id2word=id2word, passes=10)\n",
    "### END SOLUTION\n",
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VgeK3rTfo_Cs"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert lda.num_topics == 3 and lda.passes == 10, \"The output should be a fitted LDA model with three topics and 10 passes.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBjK8sY1mmeE"
   },
   "source": [
    "Let's print the top words of each of the 3 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDAVg2iXmW8o",
    "outputId": "b4205f54-a8b8-4768-f77a-12b1194d6539"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 17:14:59,631 : INFO : topic #0 (0.333): 0.004*\"bike\" + 0.002*\"just\" + 0.002*\"like\" + 0.001*\"good\" + 0.001*\"don\" + 0.001*\"got\" + 0.001*\"helmet\" + 0.001*\"know\" + 0.001*\"way\" + 0.001*\"dod\"\n",
      "2021-11-11 17:14:59,633 : INFO : topic #1 (0.333): 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"know\" + 0.002*\"motorcycle\" + 0.001*\"bikes\" + 0.001*\"right\" + 0.001*\"bmw\" + 0.001*\"don\" + 0.001*\"think\"\n",
      "2021-11-11 17:14:59,637 : INFO : topic #2 (0.333): 0.004*\"dod\" + 0.003*\"bike\" + 0.002*\"like\" + 0.002*\"just\" + 0.002*\"ride\" + 0.002*\"don\" + 0.002*\"know\" + 0.001*\"dog\" + 0.001*\"time\" + 0.001*\"ve\"\n"
     ]
    }
   ],
   "source": [
    "lda.print_topics();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRLkI1S_nXMJ"
   },
   "source": [
    "Take a look at the top words in each of these topics. Each time you run the LDA model, the results will be slightly different because of the random initiation of topic assignments.\n",
    "\n",
    "This is the interpretation of one set of results. Yours will likely be different.\n",
    "* Topic 1: certain brands of bikes\n",
    "* Topic 2: new bikes and helments\n",
    "* Topic 3: good times riding bikes\n",
    "\n",
    "The results may look fuzzy though, so to clean them up, you have several options:\n",
    "* Increase the number of passes to get more stable results.\n",
    "* Change the number of topics.\n",
    "* Clean up the text more in the CountVectorizer step, such as adding to the stop word list, removing common words, etc.\n",
    "\n",
    "Spend a few minutes doing at least one of these things to make your model better before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ua0e-B-33j1"
   },
   "source": [
    "After you are satisfied with final topics, your task is to figure out which topics are in each document. Transform the original `doc_term` matrix into a document-topic matrix and save it as `doc_topic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5uyk9hFkt6hA",
    "outputId": "575dd1c8-6c4a-47ba-e685-f5b6138a6b21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 0.99593365)],\n",
       " [(0, 0.030585783), (1, 0.028712576), (2, 0.9407016)],\n",
       " [(0, 0.33333334), (1, 0.33333334), (2, 0.33333334)],\n",
       " [(2, 0.9843929)],\n",
       " [(1, 0.9939523)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "doc_topic = [doc for doc in lda[corpus]]\n",
    "### END SOLUTION\n",
    "doc_topic[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NQiz0hTs9XMI"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert len(doc_topic) == 598, \"The doc_topic matrix should have 598 items.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo4gtlSFv3h0"
   },
   "source": [
    "Let's take a look at a document and see if the topic distribution makes sense.\n",
    "\n",
    "The list of tuples is the list of topics in the document. The first value of a tuple is the topic, and the second value is the percent of the document that is that topic. If a topic is less than a percent of a document, that tuple is left out.\n",
    "\n",
    "Display the document distribution for the 0th document and name it `doc_0_topics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlhxL1VQ-PJ4",
    "outputId": "2104ba5f-8512-4e0b-8713-8125ccd967fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.99593365)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "doc_0_topics = doc_topic[0]\n",
    "### END SOLUTION\n",
    "doc_0_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_lCen0dS3azn"
   },
   "outputs": [],
   "source": [
    "### CHECK YOUR OUTPUT WITH THE ANSWER\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert type(doc_0_topics) == list, \"The doc_0_topics variable should be a list.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRDHpIKq3r76"
   },
   "source": [
    "Take a look at the full text for the document. Do the topic assignments make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "2-2A9zyuv_gx",
    "outputId": "453a7b2d-c7fd-4016-88e1-dce0f9e27eb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Now, I am jumping into the middle of this thread so I may not know\\nwhat y'all been talking about, but I have a few comments:\\n\\n\\nThere are a number of other factors that are very important, the three\\nbiggest being air velocity, air momentum and shock waves.\\nVelocity stacks have been used for years and are now being used inside\\nof stock airboxes on a number of bikes.  At a tuned engine rpm, the\\nstacks can greatly increase the speed, and thus momentum of the air\\nrushing in.\\nAir momentum is critical in getting good air intake: the momentum of\\nthe air stack outside the combustion chamber will force its way inside\\nlong after the piston has begun its compressive up-stroke.\\nShock waves are used to induce air intake and to prevent fresh air from\\nescaping out the exzhaust ports.  Shock waves are the product of expansion\\nchambers or any other means of presenting a 'wall' (opening or closing)\\nto the air in motion.  Beyond this I am lost in the mystery of how they\\ndesign for shock waves.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYTeC2zD4EXT"
   },
   "source": [
    "If it makes some sense, great! If it doesn't, that indicates that you'll need to further tune your topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "804aQkSu09oc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nlp-unsupervised-topic-modeling-exercises4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
